#!/bin/sh
set -e

# Persistent storage root (S3 mount or Docker volume)
DATA_DIR="/data"
SQLITE_LOCAL_DIR="/data-local/sqlite"

# --- Derive keys from MASTER_KEY via HKDF-SHA256 ---
# One master secret derives: rclone crypt password, crypt salt, gateway auth token.
# S3 credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) are provider-issued and stay separate.
if [ -n "$MASTER_KEY" ]; then
  echo "Deriving keys from MASTER_KEY..."
  derive_key() {
    node -e "
      const c = require('crypto');
      const key = c.hkdfSync('sha256', process.argv[1], '', process.argv[2], 32);
      process.stdout.write(Buffer.from(key).toString('base64'));
    " "$MASTER_KEY" "$1"
  }

  # Derive and obscure rclone crypt passwords (rclone needs its own obscured format)
  RCLONE_CRYPT_PASSWORD=$(rclone obscure "$(derive_key rclone-crypt-password)")
  RCLONE_CRYPT_PASSWORD2=$(rclone obscure "$(derive_key rclone-crypt-salt)")
  GATEWAY_AUTH_TOKEN=$(derive_key gateway-auth-token | tr -d '/+=' | head -c 32)

  export RCLONE_CRYPT_PASSWORD RCLONE_CRYPT_PASSWORD2 GATEWAY_AUTH_TOKEN
  echo "Keys derived (crypt password, crypt salt, gateway token)."
fi

# --- Encrypted S3 storage via rclone crypt + mount ---
if [ -n "$S3_BUCKET" ]; then
  echo "S3 storage configured (bucket: $S3_BUCKET), setting up rclone..."

  S3_PREFIX="${S3_PREFIX:-openclaw-data}"
  S3_REGION="${S3_REGION:-us-east-1}"

  # Generate rclone config from env vars (write to temp location, not ~/.config)
  mkdir -p /tmp/rclone
  cat > /tmp/rclone/rclone.conf <<RCONF
[s3]
type = s3
provider = ${S3_PROVIDER:-Other}
env_auth = true
endpoint = ${S3_ENDPOINT}
region = ${S3_REGION}
no_check_bucket = true

[s3-crypt]
type = crypt
remote = s3:${S3_BUCKET}/${S3_PREFIX}
password = ${RCLONE_CRYPT_PASSWORD}
password2 = ${RCLONE_CRYPT_PASSWORD2:-}
filename_encryption = standard
directory_name_encryption = true
RCONF
  export RCLONE_CONFIG=/tmp/rclone/rclone.conf

  # Try FUSE mount first; fall back to rclone sync if FUSE unavailable
  mkdir -p "$DATA_DIR"
  S3_MODE=""

  if [ -e /dev/fuse ]; then
    echo "Attempting FUSE mount..."
    # Unmount Docker volume if present (FUSE can't overlay on existing mounts)
    if mountpoint -q "$DATA_DIR" 2>/dev/null; then
      echo "Unmounting existing volume at $DATA_DIR..."
      umount "$DATA_DIR" 2>/dev/null || true
    fi
    rclone mount s3-crypt: "$DATA_DIR" \
      --config "$RCLONE_CONFIG" \
      --vfs-cache-mode writes \
      --vfs-write-back 5s \
      --dir-cache-time 30s \
      --vfs-cache-max-size 500M \
      --allow-other \
      --daemon 2>&1 || true

    # Wait for FUSE mount (up to 10s) — check for fuse.rclone specifically,
    # not just mountpoint (Docker volume is already a mountpoint)
    MOUNT_WAIT=0
    while ! mount | grep -q "on $DATA_DIR type fuse.rclone"; do
      sleep 0.5
      MOUNT_WAIT=$((MOUNT_WAIT + 1))
      if [ $MOUNT_WAIT -ge 20 ]; then
        break
      fi
    done

    if mount | grep -q "on $DATA_DIR type fuse.rclone"; then
      S3_MODE="mount"
      echo "rclone FUSE mount ready at $DATA_DIR"
    else
      echo "FUSE mount failed, falling back to sync mode."
    fi
  fi

  # Fallback: sync mode (pull from S3, periodic push back)
  if [ -z "$S3_MODE" ]; then
    S3_MODE="sync"
    echo "Using rclone sync mode (no FUSE)."
    # Restore SQLite files to local storage (can't run on FUSE, use symlinks instead)
    mkdir -p "$SQLITE_LOCAL_DIR"
    echo "Restoring SQLite files from S3..."
    rclone copy s3-crypt:sqlite/ "$SQLITE_LOCAL_DIR/" --config "$RCLONE_CONFIG" 2>/dev/null || true
    # Pull remaining state
    rclone copy s3-crypt: "$DATA_DIR/" --config "$RCLONE_CONFIG" --exclude "sqlite/**" 2>&1 || true
    echo "Initial sync from S3 complete."
  fi

  # In sync mode, run periodic background jobs to push changes to S3.
  # In mount mode, rclone VFS cache handles syncing automatically.
  if [ "$S3_MODE" = "sync" ]; then
    (
      while true; do
        sleep 60
        rclone copy "$SQLITE_LOCAL_DIR/" s3-crypt:sqlite/ --config "$RCLONE_CONFIG" 2>/dev/null || true
        rclone copy "$DATA_DIR/" s3-crypt: --config "$RCLONE_CONFIG" --exclude "sqlite/**" 2>/dev/null || true
      done
    ) &
    echo "Background sync started (PID $!)"
  fi
else
  mkdir -p "$DATA_DIR"
fi

# --- Set up home directory symlinks ---
# ~/.openclaw → /data/openclaw (state dir)
# ~/.config → /data/.config (plugin configs)
mkdir -p "$DATA_DIR/openclaw" "$DATA_DIR/.config"
ln -sfn "$DATA_DIR/openclaw" /root/.openclaw
ln -sfn "$DATA_DIR/.config" /root/.config
echo "Home symlinks created (~/.openclaw, ~/.config → $DATA_DIR)"

# Symlink Codex auth directory for OAuth token persistence
mkdir -p "$DATA_DIR/codex-auth"
rm -rf /root/.codex
ln -s "$DATA_DIR/codex-auth" /root/.codex
echo "Symlinked /root/.codex -> $DATA_DIR/codex-auth"

# Bootstrap config if none exists — generates full Redpill provider + model catalog + Codex
CONFIG_FILE="/root/.openclaw/openclaw.json"
if [ ! -f "$CONFIG_FILE" ]; then
  BOOT_TOKEN="${GATEWAY_AUTH_TOKEN:-$(head -c 32 /dev/urandom | base64 | tr -d '/+=' | head -c 32)}"
  export CONFIG_FILE BOOT_TOKEN
  node -e "
    // Import Redpill config functions from installed openclaw package
    const { writeFileSync } = require('fs');
    const PKG = 'file:///usr/lib/node_modules/openclaw/dist/commands/onboard-auth.config-core.js';

    (async () => {
      const { applyRedpillConfig } = await import(PKG);

      const base = {
        gateway: {
          mode: 'local',
          bind: 'lan',
          auth: { token: process.env.BOOT_TOKEN },
          controlUi: { dangerouslyDisableDeviceAuth: true },
        },
        update: { checkOnStart: false },
        agents: {
          defaults: {
            memorySearch: {
              provider: 'openai',
              model: 'qwen/qwen3-embedding-8b',
              remote: { baseUrl: 'https://api.redpill.ai/v1', apiKey: process.env.REDPILL_API_KEY || undefined },
              fallback: 'none',
            },
          },
        },
      };

      // Apply full Redpill provider config (model catalog + default model)
      let cfg = applyRedpillConfig(base);

      // --- OPENAI CODEX PROVIDER (OAuth — uses ChatGPT subscription, no API key) ---
      // Uses chatgpt.com/backend-api with openai-codex-responses API type
      // This works with ChatGPT OAuth tokens (no api.responses.write scope needed)
      cfg.models.providers['openai-codex'] = {
        baseUrl: 'https://chatgpt.com/backend-api',
        api: 'openai-codex-responses',
        models: [
          { id: 'gpt-5.2', name: 'GPT-5.2 (Codex)', reasoning: false, input: ['text', 'image'], contextWindow: 200000, maxTokens: 32768 },
          { id: 'gpt-5', name: 'GPT-5 (Codex)', reasoning: false, input: ['text', 'image'], contextWindow: 200000, maxTokens: 32768 },
        ]
      };

      // Add model aliases for Codex
      cfg.agents.defaults.models = cfg.agents.defaults.models || {};
      cfg.agents.defaults.models['openai-codex/gpt-5.2'] = { alias: 'codex' };
      cfg.agents.defaults.models['openai-codex/gpt-5'] = { alias: 'codex5' };

      // --- ANTHROPIC PROVIDER (fallback) ---
      if (process.env.ANTHROPIC_API_KEY) {
        cfg.models.providers['anthropic'] = {
          baseUrl: 'https://api.anthropic.com',
          apiKey: process.env.ANTHROPIC_API_KEY,
          models: [
            { id: 'claude-opus-4-5-20251101', name: 'Claude Opus 4.5', reasoning: false, input: ['text', 'image'], contextWindow: 200000, maxTokens: 32000 },
            { id: 'claude-sonnet-4-20250514', name: 'Claude Sonnet 4', reasoning: false, input: ['text', 'image'], contextWindow: 200000, maxTokens: 64000 },
          ]
        };
        cfg.agents.defaults.models['anthropic/claude-opus-4-5-20251101'] = { alias: 'opus' };
        cfg.agents.defaults.models['anthropic/claude-sonnet-4-20250514'] = { alias: 'sonnet' };
      }

      // Override default model to Codex GPT-5.2
      cfg.agents.defaults.model.primary = 'openai-codex/gpt-5.2';

      // Inject Redpill API key if provided
      if (process.env.REDPILL_API_KEY) {
        cfg.models.providers.redpill.apiKey = process.env.REDPILL_API_KEY;
      }

      writeFileSync(process.env.CONFIG_FILE, JSON.stringify(cfg, null, 2));
    })().catch((err) => {
      console.error(err);
      process.exit(1);
    });
  " 2>&1 || {
    # Fallback: write minimal config if node import fails (e.g. package structure changed)
    echo "Warning: full config generation failed, writing minimal config."
    cat > "$CONFIG_FILE" <<CONF
{"gateway":{"mode":"local","bind":"lan","auth":{"token":"$BOOT_TOKEN"},"controlUi":{"dangerouslyDisableDeviceAuth":true}},"update":{"checkOnStart":false},"agents":{"defaults":{"model":{"primary":"openai-codex/gpt-5.2"},"memorySearch":{"provider":"openai","model":"qwen/qwen3-embedding-8b","remote":{"baseUrl":"https://api.redpill.ai/v1"},"fallback":"none"}}},"models":{"providers":{"openai-codex":{"baseUrl":"https://chatgpt.com/backend-api","api":"openai-codex-responses","models":[{"id":"gpt-5.2","name":"GPT-5.2 (Codex)"}]}}}}
CONF
  }
  echo "Created config at $CONFIG_FILE (token: ${GATEWAY_AUTH_TOKEN:+derived}${GATEWAY_AUTH_TOKEN:-random})"
fi

# --- SQLite symlink helper ---
# Called after gateway creates agent dirs to redirect memory.db to local storage
setup_sqlite_symlinks() {
  if [ -z "$S3_BUCKET" ]; then return; fi
  for agent_dir in /root/.openclaw/agents/*/; do
    [ -d "$agent_dir" ] || continue
    agent_id=$(basename "$agent_dir")
    local_db="$SQLITE_LOCAL_DIR/${agent_id}-memory.db"
    target_db="${agent_dir}memory.db"
    # If real file exists on mount, move it to local
    if [ -f "$target_db" ] && [ ! -L "$target_db" ]; then
      cp "$target_db" "$local_db" 2>/dev/null || true
      rm -f "$target_db"
    fi
    # Create symlink if not already there
    if [ ! -L "$target_db" ]; then
      # Ensure local db exists (may have been restored from S3)
      touch "$local_db"
      ln -sf "$local_db" "$target_db"
    fi
  done
}

# Start SSH daemon first (always available for debugging)
mkdir -p /var/run/sshd /root/.ssh
chmod 700 /root/.ssh 2>/dev/null || true
chmod 600 /root/.ssh/authorized_keys 2>/dev/null || true
/usr/sbin/sshd
echo "SSH daemon started."

# Clean up stale PID files from previous container restarts
rm -f /var/run/docker.pid /var/run/containerd/containerd.pid

# Start Docker daemon in background (best-effort, not critical for gateway)
dockerd --host=unix:///var/run/docker.sock --storage-driver=vfs &
DOCKERD_PID=$!

echo "Waiting for Docker daemon..."
DOCKER_WAIT=0
while ! docker info >/dev/null 2>&1; do
  sleep 1
  DOCKER_WAIT=$((DOCKER_WAIT + 1))
  if [ $DOCKER_WAIT -ge 30 ]; then
    echo "Warning: Docker daemon not ready after 30s, continuing without it."
    break
  fi
  # Check if dockerd process died
  if ! kill -0 $DOCKERD_PID 2>/dev/null; then
    echo "Warning: Docker daemon exited, continuing without it."
    break
  fi
done
if docker info >/dev/null 2>&1; then
  echo "Docker daemon ready."
fi

# Set up SQLite symlinks — only needed in sync mode (no VFS cache).
# In FUSE mount mode, --vfs-cache-mode writes handles SQLite locally.
if [ "$S3_MODE" = "sync" ]; then
  setup_sqlite_symlinks
  (
    while true; do
      sleep 30
      setup_sqlite_symlinks
    done
  ) &
fi

# --- Patch OpenClaw to accept openai-codex-responses API type ---
# The pi-ai library supports openai-codex-responses but OpenClaw's config validation
# schema doesn't include it. This patches the validation to accept it.
echo "Patching OpenClaw validation for openai-codex-responses..."
OPENCLAW_DIST="/usr/lib/node_modules/openclaw/dist"
for f in "$OPENCLAW_DIST"/*.js "$OPENCLAW_DIST"/commands/*.js "$OPENCLAW_DIST"/plugin-sdk/*.js; do
  [ -f "$f" ] || continue
  if grep -q 'z\.literal("bedrock-converse-stream")' "$f" 2>/dev/null; then
    sed -i 's/z\.literal("bedrock-converse-stream")/z.literal("bedrock-converse-stream"),z.literal("openai-codex-responses")/g' "$f"
  fi
done
echo "OpenClaw validation patched."

# Start openclaw gateway (foreground)
exec openclaw gateway run --bind lan --port 18789 --force
